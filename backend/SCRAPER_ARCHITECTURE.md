# Архитектура системы парсинга событий

## Обзор

Система парсинга событий полностью переработана для обеспечения стабильности, скорости и готовности к продакшену.

## Структура файлов

```
backend/app/
├── utils/
│   └── http_client.py          # HTTP клиент с таймаутами и ретраями
├── services/
│   └── scraper_service.py      # Сервис для запуска всех скраперов
├── cron/
│   └── scraper_job.py          # CRON задача для периодического парсинга
├── scrapers/
│   ├── base_scraper.py         # Базовый класс для всех скраперов
│   ├── astana_hub_scraper.py   # Скрапер Astana Hub
│   ├── nu_scraper.py           # Скрапер Nazarbayev University
│   ├── aitu_scraper.py         # Скрапер AITU
│   ├── nfactorial_scraper.py   # Скрапер nFactorial
│   ├── techorda_scraper.py     # Скрапер TechOrda
│   └── scheduler.py            # Планировщик CRON задач
└── api/routers/
    └── events.py               # API endpoint (только чтение из БД)
```

## Ключевые особенности

### 1. CRON-задачи
- Парсинг запускается автоматически каждые **5 минут** в фоновом режиме
- Не блокирует фронтенд запросы
- Логирует все ошибки, но не останавливает процесс

### 2. Параллельный парсинг
- Все скраперы запускаются одновременно через `asyncio.gather()`
- Если один скрапер падает - остальные продолжают работу
- Каждый скрапер возвращает количество обработанных событий или 0

### 3. Таймауты и ретраи
- HTTP таймаут: **5 секунд** на запрос
- Автоматические ретраи: **3 попытки** с экспоненциальной задержкой
- Если сайт не отвечает - скрапер пропускается, не ломая весь процесс

### 4. Обработка ошибок
- Все скраперы обернуты в try/catch
- Ошибки логируются, но не пробрасываются
- Возвращается пустой массив вместо исключения
- API endpoint возвращает пустой список вместо ошибки

### 5. Кеширование
- API endpoint `/api/events/` **только читает** из БД
- Парсинг **НЕ выполняется** при запросах с фронтенда
- Если данных нет - возвращается пустой список (не ошибка)
- Время ответа < 100ms (быстрый запрос к БД)

### 6. Логирование
- Все ошибки логируются на бэкенде
- Ошибки **НЕ передаются** на фронтенд
- Детальные логи для отладки

## Поток данных

```
1. CRON (каждые 5 минут)
   ↓
2. scraper_service.run_all_scrapers()
   ↓
3. Параллельный запуск всех скраперов
   ↓
4. Каждый скрапер:
   - Получает HTML (с ретраями)
   - Парсит события
   - Сохраняет в БД
   ↓
5. Результаты логируются
   ↓
6. Фронтенд запрашивает события
   ↓
7. API читает из БД (кеш)
   ↓
8. Возвращает события (< 100ms)
```

## API Endpoints

### GET `/api/events/`
- **Назначение**: Получить события из кеша (БД)
- **Парсинг**: НЕ выполняется
- **Время ответа**: < 100ms
- **Ошибки**: Возвращает пустой список вместо ошибки

### POST `/api/admin/run-scraper` (только для админов)
- **Назначение**: Ручной запуск парсинга
- **Использование**: Только для админов, в основном для тестирования

## Конфигурация

### Таймауты
- HTTP запрос: 5 секунд
- Ретраи: 3 попытки
- Задержка между ретраями: 1 секунда (экспоненциальная)

### CRON расписание
- Интервал: 5 минут
- Запуск при старте: Да (в фоне)

## Мониторинг

Все операции логируются:
- Успешные парсинги
- Ошибки скраперов
- Количество обработанных событий
- Время выполнения

## Стабильность

- ✅ Нет блокирующих операций на фронтенде
- ✅ Все ошибки обрабатываются
- ✅ Параллельный парсинг не блокирует другие скраперы
- ✅ Быстрый ответ API (< 100ms)
- ✅ Готовность к продакшену

