import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Optional
from urllib.parse import urljoin

from bs4 import BeautifulSoup
from sqlalchemy import and_, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.db import AsyncSessionLocal
from app.models.event import Event
from app.repositories.event_repository import EventRepository
from app.utils.http_client import HttpClient

logger = logging.getLogger(__name__)


class BaseScraper(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Å–∫—Ä–∞–ø–µ—Ä–æ–≤ —Å–æ–±—ã—Ç–∏–π"""

    def __init__(self, name: str, base_url: str):
        self.name = name
        self.base_url = base_url
        self.event_repo = EventRepository()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º HttpClient —Å —Ç–∞–π–º–∞—É—Ç–æ–º 5 —Å–µ–∫—É–Ω–¥ –∏ 3 —Ä–µ—Ç—Ä–∞—è–º–∏
        self.http_client = HttpClient(
            timeout=5.0,
            max_retries=3,
            retry_delay=1.0,
        )

    async def fetch_html(self, url: str) -> Optional[str]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ—Ç—Ä–∞—è–º–∏.
        
        Returns:
            str: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        return await self.http_client.get(url)

    @abstractmethod
    def parse(self, html: str) -> list[dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç HTML –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–±—ã—Ç–∏–π"""
        pass

    def normalize_event(self, data: dict[str, Any]) -> dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î"""
        normalized = {
            "title": data.get("title", "").strip(),
            "description": data.get("description", "").strip() or None,
            "date_start": data.get("start_date"),
            "date_end": data.get("end_date"),
            "city": data.get("location", "").strip() or None,
            "type": data.get("category", "other"),
            "banner": data.get("image_url") or None,
            "source": "external",
            "source_url": data.get("source_url", "").strip() or None,
            "organizer_id": None,  # –í–Ω–µ—à–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è –Ω–µ –∏–º–µ—é—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞
            "is_online": data.get("is_online", False),
            "tags": data.get("tags", []),
        }

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        if not normalized["title"]:
            raise ValueError("Event title is required")
        if not normalized["date_start"]:
            raise ValueError("Event start_date is required")
        if not isinstance(normalized["date_start"], datetime):
            raise ValueError("start_date must be a datetime object")

        return normalized

    async def save_or_update(self, event_data: dict[str, Any], db: AsyncSession) -> Event:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ –ë–î (UPSERT)"""
        try:
            normalized = self.normalize_event(event_data)

            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–æ–±—ã—Ç–∏–µ –ø–æ source_url –∏–ª–∏ title + start_date
            conditions = [Event.source == "external"]
            
            if normalized["source_url"]:
                # –ï—Å–ª–∏ –µ—Å—Ç—å source_url, –∏—â–µ–º –ø–æ –Ω–µ–º—É
                conditions.append(Event.source_url == normalized["source_url"])
            else:
                # –ò–Ω–∞—á–µ –∏—â–µ–º –ø–æ title + start_date
                conditions.append(Event.title == normalized["title"])
                conditions.append(Event.date_start == normalized["date_start"])
            
            stmt = select(Event).where(and_(*conditions))
            result = await db.execute(stmt)
            existing_event = result.scalar_one_or_none()

            if existing_event:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–æ–±—ã—Ç–∏–µ
                logger.info(f"[{self.name}] Updating event: {normalized['title']}")
                for key, value in normalized.items():
                    setattr(existing_event, key, value)
                await db.flush()
                await db.refresh(existing_event)
                return existing_event
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ
                logger.info(f"[{self.name}] Creating new event: {normalized['title']}")
                event = await self.event_repo.create(db, normalized)
                return event

        except Exception as e:
            logger.error(f"[{self.name}] Error saving event: {e}")
            raise

    async def scrape(self) -> int:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏–ª–∏ 0 –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        logger.info(f"[{self.name}] üöÄ Starting scraping from {self.base_url}")
        count = 0

        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ—Ç—Ä–∞—è–º–∏
            html = await self.fetch_html(self.base_url)
            
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
            if not html:
                logger.warning(f"[{self.name}] ‚ö†Ô∏è Failed to fetch HTML, skipping...")
                return 0
            
            logger.info(f"[{self.name}] ‚úÖ HTML fetched, length: {len(html)} chars")
            
            # –ü–∞—Ä—Å–∏–º —Å–æ–±—ã—Ç–∏—è
            try:
                events_data = self.parse(html)
                logger.info(f"[{self.name}] ‚úÖ Parsed {len(events_data)} events from HTML")
            except Exception as e:
                logger.error(f"[{self.name}] ‚ùå Error parsing HTML: {e}", exc_info=True)
                return 0

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–±—ã—Ç–∏—è –≤ –ë–î
            if events_data:
                async with AsyncSessionLocal() as db:
                    for i, event_data in enumerate(events_data, 1):
                        try:
                            await self.save_or_update(event_data, db)
                            count += 1
                            if i <= 3:  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                                logger.info(f"[{self.name}] üíæ Saved event {i}: {event_data.get('title', 'Unknown')[:50]}")
                        except Exception as e:
                            logger.error(f"[{self.name}] ‚ùå Error processing event {i}: {e}")
                            continue

                    await db.commit()
                    logger.info(f"[{self.name}] ‚úÖ Successfully saved {count}/{len(events_data)} events to DB")
            else:
                logger.warning(f"[{self.name}] ‚ö†Ô∏è No events found in HTML")

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Error during scraping: {e}", exc_info=True)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 0 –≤–º–µ—Å—Ç–æ –ø—Ä–æ–±—Ä–æ—Å–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            return 0

        return count

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP –∫–ª–∏–µ–Ω—Ç"""
        if hasattr(self, "http_client"):
            try:
                await self.http_client.close()
            except Exception as e:
                logger.warning(f"[{self.name}] Error closing HTTP client: {e}")

